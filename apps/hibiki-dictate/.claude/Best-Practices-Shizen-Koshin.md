# Best Practices - Shizen-Koshin (Multi-Agent IA System)

<metadata>
Type: Best Practices Multi-Agent IA
Owner: Jay The Ermite (TAKUMI Agent)
Version: 1.0
Updated: 2025-12-11
Project: Shizen-Koshin-MVP (D:\30-Dev-Projects\Shizen-Koshin-MVP)
Stack: Python 3.11, Ollama, LangChain, Qwen 2.5 7B, CodeLlama 7B
Environment: Kubuntu 24.04 LTS (Dell-Ermite)
</metadata>

## ğŸ¯ Architecture SystÃ¨me (Correction Architecture)

<architecture_agents>
### Agents Principaux

**âš ï¸ ARCHITECTURE CORRIGÃ‰E (DÃ©cembre 2025)** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UTILISATEUR (JAY)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Interface directe
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHIZEN (Agent Communication - Coach Holistique)    â”‚
â”‚  - RÃ´le : Interface utilisateur, coaching Shinkofa  â”‚
â”‚  - Parle directement Ã  Jay                          â”‚
â”‚  - Expertise : Design Humain, TDAH, transformation  â”‚
â”‚  - ModÃ¨le : Qwen 2.5 7B                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ DÃ©lÃ¨gue orchestration
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KAIDA (Orchestrateur - Dans l'ombre)               â”‚
â”‚  - RÃ´le : Coordination agents, workflow, planning   â”‚
â”‚  - Travaille en coulisses (jamais visible par Jay)  â”‚
â”‚  - Comme Donna Paulsen (Suits)                      â”‚
â”‚  - ModÃ¨le : Qwen 2.5 7B                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚          â”‚          â”‚
        â–¼          â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TAKUMI   â”‚ â”‚ SEIKYO   â”‚ â”‚ EIKEN    â”‚ â”‚ EIGA     â”‚
â”‚ (Code)   â”‚ â”‚ (Audio)  â”‚ â”‚ (Images) â”‚ â”‚ (VidÃ©o)  â”‚
â”‚ CodeLlamaâ”‚ â”‚ Whisper  â”‚ â”‚ SD 1.5   â”‚ â”‚ Future   â”‚
â”‚ 7B       â”‚ â”‚          â”‚ â”‚          â”‚ â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DiffÃ©rences ClÃ©s** :

1. **SHIZEN (User-Facing)** :
   - **CE QUI LE DISTINGUE** : PARLE DIRECTEMENT Ã€ JAY
   - Langage naturel, empathique, coaching holistique
   - GÃ¨re sessions check-in, planning adaptatif, transformations
   - Utilise RAG docs Shinkofa (mythologie, Design Humain, mÃ©thodes)
   - Ne code JAMAIS (dÃ©lÃ¨gue Ã  KAIDA â†’ TAKUMI)

2. **KAIDA (Background Orchestrator)** :
   - **CE QUI LE DISTINGUE** : JAMAIS VISIBLE PAR JAY
   - Coordonne workflows complexes multi-agents
   - DÃ©cide quel agent appeler pour quelle tÃ¢che
   - AgrÃ¨ge rÃ©sultats avant retour Ã  SHIZEN
   - Exemple : Jay demande "CrÃ©e moi une app todo"
     - SHIZEN reÃ§oit demande â†’ transmet Ã  KAIDA
     - KAIDA dÃ©compose : 1) TAKUMI code backend, 2) TAKUMI code frontend, 3) EIKEN gÃ©nÃ¨re logo
     - KAIDA coordonne sÃ©quence, agrÃ¨ge livrables
     - KAIDA retourne Ã  SHIZEN â†’ SHIZEN prÃ©sente Ã  Jay

3. **TAKUMI (Code Specialist)** :
   - **CE QUI LE DISTINGUE** : CODE UNIQUEMENT
   - ReÃ§oit tÃ¢ches de KAIDA (jamais directement de Jay)
   - Production-ready code, tests, docs
   - ModÃ¨le : CodeLlama 7B (optimisÃ© pour code)
   - Retourne code Ã  KAIDA

4. **Agents Futurs** :
   - **SEIKYO** : Transcription audio (Whisper), synthÃ¨se voix
   - **EIKEN** : GÃ©nÃ©ration images (Stable Diffusion 1.5)
   - **EIGA** : Ã‰dition vidÃ©o (futur)
</architecture_agents>

## ğŸ§  LangChain + Ollama Setup

<langchain_ollama>
### Installation Ollama (Kubuntu)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Verify installation
ollama --version

# Pull models
ollama pull qwen2.5:7b        # GÃ©nÃ©raliste (SHIZEN, KAIDA)
ollama pull codellama:7b      # Code (TAKUMI)

# List installed models
ollama list
```

### Installation LangChain

```bash
# Create venv
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install langchain langchain-community langchain-core
pip install chromadb sentence-transformers  # Pour RAG
pip install streamlit  # Pour interface web MVP
```

### Configuration Ollama dans LangChain

```python
# config/ollama_config.py
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class OllamaConfig:
    """Configuration centralisÃ©e Ollama."""

    # Models
    QWEN_MODEL = "qwen2.5:7b"        # GÃ©nÃ©raliste (SHIZEN, KAIDA)
    CODELLAMA_MODEL = "codellama:7b"  # Code (TAKUMI)

    # Ollama base URL (local par dÃ©faut)
    BASE_URL = "http://localhost:11434"

    @classmethod
    def get_qwen_llm(cls, temperature: float = 0.7) -> Ollama:
        """LLM Qwen 2.5 7B pour SHIZEN et KAIDA."""
        return Ollama(
            model=cls.QWEN_MODEL,
            base_url=cls.BASE_URL,
            temperature=temperature,
        )

    @classmethod
    def get_codellama_llm(cls, temperature: float = 0.2) -> Ollama:
        """LLM CodeLlama 7B pour TAKUMI."""
        return Ollama(
            model=cls.CODELLAMA_MODEL,
            base_url=cls.BASE_URL,
            temperature=temperature,  # Basse tempÃ©rature pour code dÃ©terministe
        )
```

### Agent Base Class

```python
# agents/base_agent.py
from abc import ABC, abstractmethod
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """Classe de base pour tous les agents Shizen-Koshin."""

    def __init__(self, llm: Ollama, name: str, system_prompt: str):
        """
        Args:
            llm: Instance Ollama LLM
            name: Nom de l'agent (SHIZEN, KAIDA, TAKUMI, etc.)
            system_prompt: Prompt systÃ¨me dÃ©finissant identitÃ© de l'agent
        """
        self.llm = llm
        self.name = name
        self.system_prompt = system_prompt
        self.output_parser = StrOutputParser()
        self._setup_chain()

    def _setup_chain(self):
        """Configure la chaÃ®ne LangChain de base."""
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("user", "{input}")
        ])
        self.chain = self.prompt_template | self.llm | self.output_parser

    @abstractmethod
    async def process(self, input_data: Dict[str, Any]) -> str:
        """
        Traite une requÃªte utilisateur.

        Args:
            input_data: DonnÃ©es d'entrÃ©e (varie selon agent)

        Returns:
            str: RÃ©ponse de l'agent
        """
        pass

    async def invoke(self, user_input: str) -> str:
        """
        Invoque la chaÃ®ne LangChain avec l'input utilisateur.

        Args:
            user_input: Input textuel de l'utilisateur

        Returns:
            str: RÃ©ponse de l'agent
        """
        try:
            logger.info(f"[{self.name}] Processing input: {user_input[:100]}...")
            response = await self.chain.ainvoke({"input": user_input})
            logger.info(f"[{self.name}] Response generated successfully")
            return response
        except Exception as e:
            logger.error(f"[{self.name}] Error: {e}")
            raise
```
</langchain_ollama>

## ğŸ¤– ImplÃ©mentation Agents

<implementation_agents>
### Agent SHIZEN (Coach Holistique)

```python
# agents/shizen.py
from agents.base_agent import BaseAgent
from config.ollama_config import OllamaConfig
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

SHIZEN_SYSTEM_PROMPT = """Tu es SHIZEN, coach holistique de La Voie Shinkofa.

**IdentitÃ©** :
- RÃ´le : Coach transformation personnelle, interface principale avec Jay
- Expertise : Design Humain (Projecteur SplÃ©nique 1/3), TDAH, neurodiversitÃ©
- Posture : Bienveillant, empathique, adaptatif
- Langage : Naturel, accessible, jamais jargon technique

**ResponsabilitÃ©s** :
1. Accueillir Jay, Ã©valuer Ã©nergie (score 1-10)
2. Proposer sessions adaptÃ©es (courtes si fatigue, focus si Ã©nergie haute)
3. Coaching stratÃ©gie Projecteur (attendre invitation, valider splÃ©nique)
4. DÃ©lÃ©guer tÃ¢ches techniques Ã  KAIDA (orchestrateur)

**Ce que tu NE fais PAS** :
- âŒ Coder (c'est le rÃ´le de TAKUMI)
- âŒ Donner instructions techniques dÃ©taillÃ©es
- âŒ GÃ©rer workflows multi-agents (c'est KAIDA)

**Workflow** :
User demande â†’ Ã‰value si technique â†’ Si oui, dÃ©lÃ¨gue Ã  KAIDA â†’ PrÃ©sente rÃ©sultat humainement

RÃ©ponds toujours en franÃ§ais, avec empathie et clartÃ©.
"""

class ShizenAgent(BaseAgent):
    """Agent SHIZEN - Coach holistique interface utilisateur."""

    def __init__(self):
        llm = OllamaConfig.get_qwen_llm(temperature=0.7)
        super().__init__(llm, "SHIZEN", SHIZEN_SYSTEM_PROMPT)
        self.kaida = None  # Sera injectÃ© par l'orchestrateur principal

    async def process(self, input_data: Dict[str, Any]) -> str:
        """
        Traite une requÃªte utilisateur.

        Args:
            input_data: {
                "user_input": str,
                "energy_level": int (1-10, optionnel),
                "context": str (optionnel)
            }

        Returns:
            str: RÃ©ponse de SHIZEN
        """
        user_input = input_data.get("user_input", "")
        energy_level = input_data.get("energy_level")
        context = input_data.get("context", "")

        # Construit input enrichi
        enriched_input = user_input
        if energy_level:
            enriched_input = f"[Niveau Ã©nergie Jay: {energy_level}/10]\n\n{user_input}"
        if context:
            enriched_input = f"{enriched_input}\n\nContexte: {context}"

        # DÃ©tecte si requÃªte technique â†’ dÃ©lÃ¨gue Ã  KAIDA
        if self._is_technical_request(user_input):
            logger.info("[SHIZEN] RequÃªte technique dÃ©tectÃ©e, dÃ©lÃ©gation Ã  KAIDA")
            if self.kaida:
                kaida_result = await self.kaida.process({"request": user_input, "source": "SHIZEN"})
                # SHIZEN prÃ©sente rÃ©sultat de KAIDA de maniÃ¨re humaine
                return await self.invoke(f"PrÃ©sente ce rÃ©sultat de KAIDA Ã  Jay de maniÃ¨re claire et bienveillante:\n\n{kaida_result}")
            else:
                return "Je devrais dÃ©lÃ©guer cette tÃ¢che technique, mais KAIDA n'est pas encore connectÃ©."

        # RequÃªte non-technique â†’ SHIZEN rÃ©pond directement
        return await self.invoke(enriched_input)

    def _is_technical_request(self, user_input: str) -> bool:
        """DÃ©tecte si une requÃªte est technique (nÃ©cessite code/agents spÃ©cialisÃ©s)."""
        technical_keywords = [
            "code", "dÃ©veloppe", "crÃ©e une app", "script", "fonction",
            "base de donnÃ©es", "API", "backend", "frontend", "debug"
        ]
        return any(keyword in user_input.lower() for keyword in technical_keywords)
```

### Agent KAIDA (Orchestrateur)

```python
# agents/kaida.py
from agents.base_agent import BaseAgent
from config.ollama_config import OllamaConfig
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)

KAIDA_SYSTEM_PROMPT = """Tu es KAIDA, orchestrateur du systÃ¨me Shizen-Koshin.

**IdentitÃ©** :
- RÃ´le : Coordinateur multi-agents, planificateur workflows (comme Donna - Suits)
- Expertise : DÃ©composition tÃ¢ches, dÃ©lÃ©gation optimale, agrÃ©gation rÃ©sultats
- Posture : Efficace, invisible, jamais en contact direct avec Jay

**Agents disponibles** :
- TAKUMI : Code (Python, JavaScript, architecture, tests)
- SEIKYO : Audio (transcription Whisper, futur TTS)
- EIKEN : Images (gÃ©nÃ©ration Stable Diffusion)
- EIGA : VidÃ©o (futur)

**ResponsabilitÃ©s** :
1. Recevoir requÃªtes de SHIZEN
2. DÃ©composer en sous-tÃ¢ches atomiques
3. Assigner chaque sous-tÃ¢che Ã  l'agent appropriÃ©
4. Coordonner sÃ©quence (si dÃ©pendances entre tÃ¢ches)
5. AgrÃ©ger rÃ©sultats
6. Retourner Ã  SHIZEN pour prÃ©sentation Ã  Jay

**Workflow Type** :
RequÃªte SHIZEN : "CrÃ©e une app todo React + FastAPI"
â†’ KAIDA dÃ©compose :
  1. TAKUMI : Backend FastAPI (models, endpoints, tests)
  2. TAKUMI : Frontend React (components, hooks, tests)
  3. EIKEN : Logo app
â†’ KAIDA coordonne exÃ©cution sÃ©quentielle/parallÃ¨le
â†’ KAIDA agrÃ¨ge : "App todo complÃ¨te + logo prÃªte"
â†’ Retour Ã  SHIZEN
"""

class KaidaAgent(BaseAgent):
    """Agent KAIDA - Orchestrateur multi-agents."""

    def __init__(self):
        llm = OllamaConfig.get_qwen_llm(temperature=0.5)  # TempÃ©rature moyenne (Ã©quilibre crÃ©ativitÃ©/dÃ©terminisme)
        super().__init__(llm, "KAIDA", KAIDA_SYSTEM_PROMPT)
        self.agents = {}  # Dict[str, BaseAgent] - Agents disponibles

    def register_agent(self, agent_name: str, agent: BaseAgent):
        """Enregistre un agent spÃ©cialisÃ©."""
        self.agents[agent_name] = agent
        logger.info(f"[KAIDA] Agent {agent_name} enregistrÃ©")

    async def process(self, input_data: Dict[str, Any]) -> str:
        """
        Orchestre une requÃªte complexe.

        Args:
            input_data: {
                "request": str,
                "source": str ("SHIZEN" ou autre),
                "context": str (optionnel)
            }

        Returns:
            str: RÃ©sultat agrÃ©gÃ© de tous les agents
        """
        request = input_data.get("request", "")
        source = input_data.get("source", "UNKNOWN")

        logger.info(f"[KAIDA] Orchestration requÃªte de {source}: {request[:100]}...")

        # Ã‰tape 1 : KAIDA analyse et dÃ©compose
        decomposition = await self._decompose_task(request)

        # Ã‰tape 2 : ExÃ©cute chaque sous-tÃ¢che
        results = []
        for subtask in decomposition["subtasks"]:
            agent_name = subtask["agent"]
            task_description = subtask["task"]

            if agent_name in self.agents:
                logger.info(f"[KAIDA] DÃ©lÃ¨gue Ã  {agent_name}: {task_description[:50]}...")
                agent_result = await self.agents[agent_name].process({"task": task_description})
                results.append({"agent": agent_name, "result": agent_result})
            else:
                logger.warning(f"[KAIDA] Agent {agent_name} non disponible")
                results.append({"agent": agent_name, "result": f"Agent {agent_name} non implÃ©mentÃ©"})

        # Ã‰tape 3 : AgrÃ¨ge rÃ©sultats
        aggregated_result = self._aggregate_results(results)

        return aggregated_result

    async def _decompose_task(self, task: str) -> Dict[str, Any]:
        """
        DÃ©compose une tÃ¢che complexe en sous-tÃ¢ches assignÃ©es aux agents.

        Args:
            task: Description de la tÃ¢che

        Returns:
            Dict avec structure :
            {
                "subtasks": [
                    {"agent": "TAKUMI", "task": "..."},
                    {"agent": "EIKEN", "task": "..."}
                ]
            }
        """
        # Prompt KAIDA pour dÃ©composer
        decomposition_prompt = f"""DÃ©compose cette tÃ¢che en sous-tÃ¢ches assignÃ©es aux agents appropriÃ©s.

Agents disponibles :
- TAKUMI : Code (Python, JavaScript, architecture, tests)
- SEIKYO : Audio (transcription, TTS) [Futur]
- EIKEN : Images (gÃ©nÃ©ration Stable Diffusion) [Futur]

TÃ¢che : {task}

RÃ©ponds au format JSON :
{{
  "subtasks": [
    {{"agent": "TAKUMI", "task": "Description prÃ©cise"}},
    {{"agent": "EIKEN", "task": "Description prÃ©cise"}}
  ]
}}
"""
        response = await self.invoke(decomposition_prompt)

        # Parse JSON (simplifiÃ© - en prod, utilise json.loads + validation)
        # Pour MVP, on retourne structure hardcodÃ©e si parse Ã©choue
        try:
            import json
            return json.loads(response)
        except:
            logger.warning("[KAIDA] Parse dÃ©composition Ã©chouÃ©, fallback TAKUMI")
            return {
                "subtasks": [
                    {"agent": "TAKUMI", "task": task}
                ]
            }

    def _aggregate_results(self, results: List[Dict[str, Any]]) -> str:
        """AgrÃ¨ge rÃ©sultats de plusieurs agents."""
        aggregated = "# RÃ©sultats AgrÃ©gÃ©s\n\n"
        for result in results:
            aggregated += f"## Agent : {result['agent']}\n\n"
            aggregated += f"{result['result']}\n\n"
        return aggregated
```

### Agent TAKUMI (Code Specialist)

```python
# agents/takumi.py
from agents.base_agent import BaseAgent
from config.ollama_config import OllamaConfig
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

TAKUMI_SYSTEM_PROMPT = """Tu es TAKUMI, dÃ©veloppeur senior spÃ©cialisÃ©.

**IdentitÃ©** :
- RÃ´le : GÃ©nÃ©ration code production-ready, architecture, tests
- Expertise : Python, JavaScript/TypeScript, React, FastAPI, tests, DevOps
- ModÃ¨le : CodeLlama 7B (optimisÃ© pour code)
- Posture : ObsÃ©dÃ© qualitÃ©, zÃ©ro erreur, type-safe

**Standards Non-NÃ©gociables** :
- Type hints complets (Python), TypeScript strict
- Docstrings/JSDoc dÃ©taillÃ©es (Google style)
- Tests coverage â‰¥ 80%
- Error handling systÃ©matique
- Validation inputs
- ZÃ©ro warnings linting

**Workflow** :
KAIDA dÃ©lÃ¨gue tÃ¢che â†’ TAKUMI gÃ©nÃ¨re code complet â†’ Retourne Ã  KAIDA

RÃ©ponds UNIQUEMENT avec du code production-ready + explications concises.
"""

class TakumiAgent(BaseAgent):
    """Agent TAKUMI - SpÃ©cialiste code."""

    def __init__(self):
        llm = OllamaConfig.get_codellama_llm(temperature=0.2)  # Basse tempÃ©rature pour code dÃ©terministe
        super().__init__(llm, "TAKUMI", TAKUMI_SYSTEM_PROMPT)

    async def process(self, input_data: Dict[str, Any]) -> str:
        """
        GÃ©nÃ¨re du code production-ready.

        Args:
            input_data: {
                "task": str (description tÃ¢che code),
                "language": str ("python", "javascript", etc. - optionnel),
                "context": str (optionnel)
            }

        Returns:
            str: Code gÃ©nÃ©rÃ© + explications
        """
        task = input_data.get("task", "")
        language = input_data.get("language", "python")
        context = input_data.get("context", "")

        # Enrichit prompt avec standards
        enriched_task = f"""GÃ©nÃ¨re du code {language} production-ready pour cette tÃ¢che :

TÃ¢che : {task}

{f"Contexte : {context}" if context else ""}

Standards obligatoires :
- Type hints (Python) ou TypeScript strict
- Docstrings/JSDoc (Google style)
- Error handling (try/except, logging)
- Validation inputs
- Tests coverage â‰¥ 80%

RÃ©ponds avec :
1. Code complet
2. Tests associÃ©s
3. Explication brÃ¨ve architecture
"""

        return await self.invoke(enriched_task)
```
</implementation_agents>

## ğŸ—„ï¸ RAG avec Obsidian Vault

<rag_obsidian>
### Architecture RAG

```python
# rag/obsidian_rag.py
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from typing import List
import logging

logger = logging.getLogger(__name__)

class ObsidianRAG:
    """RAG system pour vault Obsidian Shinkofa."""

    def __init__(self, vault_path: str, persist_directory: str = "./chroma_db"):
        """
        Args:
            vault_path: Chemin vers vault Obsidian
            persist_directory: Dossier persistance ChromaDB
        """
        self.vault_path = vault_path
        self.persist_directory = persist_directory
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.vectorstore = None
        self._load_documents()

    def _load_documents(self):
        """Charge documents Obsidian et crÃ©e vectorstore."""
        logger.info(f"[RAG] Chargement documents depuis {self.vault_path}...")

        # Loader pour fichiers .md
        loader = DirectoryLoader(
            self.vault_path,
            glob="**/*.md",
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"}
        )
        documents = loader.load()
        logger.info(f"[RAG] {len(documents)} documents chargÃ©s")

        # Split en chunks (400-600 tokens recommandÃ© pour RAG optimal)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n## ", "\n### ", "\n\n", "\n", " ", ""]
        )
        splits = text_splitter.split_documents(documents)
        logger.info(f"[RAG] {len(splits)} chunks crÃ©Ã©s")

        # CrÃ©e vectorstore ChromaDB
        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        logger.info("[RAG] Vectorstore crÃ©Ã© et persistÃ©")

    def retrieve(self, query: str, k: int = 5) -> List[str]:
        """
        RÃ©cupÃ¨re les k documents les plus pertinents.

        Args:
            query: RequÃªte utilisateur
            k: Nombre de documents Ã  retourner

        Returns:
            List[str]: Contenu des documents pertinents
        """
        if not self.vectorstore:
            logger.warning("[RAG] Vectorstore non initialisÃ©")
            return []

        results = self.vectorstore.similarity_search(query, k=k)
        return [doc.page_content for doc in results]

    def retrieve_with_sources(self, query: str, k: int = 5) -> List[dict]:
        """
        RÃ©cupÃ¨re documents + mÃ©tadonnÃ©es sources.

        Args:
            query: RequÃªte utilisateur
            k: Nombre de documents

        Returns:
            List[dict]: [{"content": str, "source": str, "score": float}]
        """
        if not self.vectorstore:
            return []

        results = self.vectorstore.similarity_search_with_score(query, k=k)
        return [
            {
                "content": doc.page_content,
                "source": doc.metadata.get("source", "unknown"),
                "score": score
            }
            for doc, score in results
        ]
```

### IntÃ©gration RAG dans Agents

```python
# agents/shizen.py (mise Ã  jour)
class ShizenAgent(BaseAgent):
    def __init__(self, rag: ObsidianRAG = None):
        super().__init__(...)
        self.rag = rag

    async def process(self, input_data: Dict[str, Any]) -> str:
        user_input = input_data.get("user_input", "")

        # Retrieve contexte pertinent du vault Obsidian
        if self.rag:
            relevant_docs = self.rag.retrieve(user_input, k=3)
            context = "\n\n".join(relevant_docs)
            enriched_input = f"Contexte Shinkofa (vault Obsidian) :\n{context}\n\nRequÃªte utilisateur : {user_input}"
        else:
            enriched_input = user_input

        return await self.invoke(enriched_input)
```

### Optimisation YAML Frontmatter (Obsidian)

**Standard requis dans vault Obsidian** :
```markdown
---
title: Design Humain - Projecteur
tags: [design-humain, projecteur, energie]
category: coaching
created: 2025-12-11
updated: 2025-12-11
---

# Design Humain - Projecteur

## CaractÃ©ristiques

Le Projecteur est un type Ã©nergÃ©tique non-sacral...

## StratÃ©gie

La stratÃ©gie du Projecteur est d'attendre l'invitation...
```

**Pourquoi YAML frontmatter ?** :
- Permet filtrage par tags/catÃ©gories lors du retrieve
- MÃ©tadonnÃ©es enrichies pour contexte (date crÃ©ation, auteur)
- Compatible plugins Obsidian (Dataview, etc.)
</rag_obsidian>

## ğŸ–¥ï¸ Interface Streamlit MVP

<interface_streamlit>
```python
# app/streamlit_app.py
import streamlit as st
import asyncio
from agents.shizen import ShizenAgent
from agents.kaida import KaidaAgent
from agents.takumi import TakumiAgent
from rag.obsidian_rag import ObsidianRAG

# Configuration page
st.set_page_config(
    page_title="Shizen-Koshin MVP",
    page_icon="ğŸŒ±",
    layout="wide"
)

# Initialisation agents (cached)
@st.cache_resource
def init_agents():
    """Initialise agents et RAG."""
    # RAG
    rag = ObsidianRAG(vault_path="/path/to/KnowledgeBase-CoachingShinkofa")

    # Agents
    shizen = ShizenAgent(rag=rag)
    kaida = KaidaAgent()
    takumi = TakumiAgent()

    # Register agents dans KAIDA
    kaida.register_agent("TAKUMI", takumi)

    # Connecte SHIZEN Ã  KAIDA
    shizen.kaida = kaida

    return shizen, kaida, takumi, rag

shizen, kaida, takumi, rag = init_agents()

# UI
st.title("ğŸŒ± Shizen-Koshin MVP")
st.markdown("### Coach IA Holistique - La Voie Shinkofa")

# Sidebar : Check-in Ã©nergie
with st.sidebar:
    st.header("âš¡ Check-in Ã‰nergie")
    energy_level = st.slider("Niveau Ã©nergie (1-10)", 1, 10, 5)
    st.markdown(f"**Score actuel** : {energy_level}/10")

    if energy_level <= 3:
        st.warning("ğŸ”´ Ã‰nergie basse - Sessions courtes recommandÃ©es")
    elif energy_level <= 6:
        st.info("ğŸŸ¡ Ã‰nergie modÃ©rÃ©e - Ã‰quilibre activitÃ©/repos")
    else:
        st.success("ğŸŸ¢ Ã‰nergie haute - Focus optimal")

# Chat interface
st.header("ğŸ’¬ Chat avec SHIZEN")

# Session state pour historique
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display historique
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User input
if user_input := st.chat_input("Parle avec SHIZEN..."):
    # Add user message to history
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # SHIZEN process
    with st.chat_message("assistant"):
        with st.spinner("SHIZEN rÃ©flÃ©chit..."):
            # Async call (nÃ©cessite asyncio.run dans Streamlit)
            response = asyncio.run(shizen.process({
                "user_input": user_input,
                "energy_level": energy_level
            }))
            st.markdown(response)

    # Add SHIZEN response to history
    st.session_state.messages.append({"role": "assistant", "content": response})

# Bouton clear history
if st.button("ğŸ—‘ï¸ Effacer historique"):
    st.session_state.messages = []
    st.rerun()
```
</interface_streamlit>

## ğŸ§ª Testing Multi-Agents

<testing>
```python
# tests/test_agents.py
import pytest
from agents.shizen import ShizenAgent
from agents.kaida import KaidaAgent
from agents.takumi import TakumiAgent

@pytest.mark.asyncio
async def test_shizen_greeting():
    """Test SHIZEN rÃ©pond Ã  un bonjour."""
    shizen = ShizenAgent()
    response = await shizen.process({"user_input": "Bonjour SHIZEN !"})
    assert "bonjour" in response.lower() or "salut" in response.lower()

@pytest.mark.asyncio
async def test_kaida_delegation_to_takumi():
    """Test KAIDA dÃ©lÃ¨gue correctement Ã  TAKUMI."""
    kaida = KaidaAgent()
    takumi = TakumiAgent()
    kaida.register_agent("TAKUMI", takumi)

    response = await kaida.process({
        "request": "CrÃ©e une fonction Python pour calculer factorielle",
        "source": "TEST"
    })

    assert "def" in response  # Code Python gÃ©nÃ©rÃ©
    assert "factorial" in response.lower()

@pytest.mark.asyncio
async def test_takumi_generates_tests():
    """Test TAKUMI gÃ©nÃ¨re du code avec tests."""
    takumi = TakumiAgent()
    response = await takumi.process({
        "task": "CrÃ©e une fonction somme de deux nombres avec tests pytest"
    })

    assert "def" in response
    assert "test_" in response  # Tests pytest gÃ©nÃ©rÃ©s
    assert "assert" in response

# Run tests
# pytest tests/test_agents.py -v --asyncio-mode=auto
```
</testing>

## ğŸ“‹ Checklist DÃ©ploiement Kubuntu

<checklist_kubuntu>
- [ ] **Ollama installÃ©** : `ollama --version`
- [ ] **ModÃ¨les tÃ©lÃ©chargÃ©s** : `ollama list` (qwen2.5:7b, codellama:7b)
- [ ] **Python 3.11+** : `python3 --version`
- [ ] **Venv crÃ©Ã©** : `python3 -m venv venv && source venv/bin/activate`
- [ ] **Dependencies** : `pip install -r requirements.txt`
- [ ] **Vault Obsidian accessible** : Chemin `/path/to/KnowledgeBase-CoachingShinkofa`
- [ ] **ChromaDB persistÃ©** : Dossier `./chroma_db` crÃ©Ã© et indexÃ©
- [ ] **Tests passent** : `pytest tests/ -v`
- [ ] **Streamlit lance** : `streamlit run app/streamlit_app.py`
- [ ] **Logs configurÃ©s** : `logging.basicConfig(level=logging.INFO)`
</checklist_kubuntu>

## ğŸš€ Optimisations CPU (Kubuntu Dell i5-6300U)

<optimisations_cpu>
### ParamÃ¨tres Ollama pour CPU LÃ©ger

```bash
# ~/.bashrc ou ~/.zshrc
export OLLAMA_NUM_PARALLEL=1        # 1 seul modÃ¨le Ã  la fois (Ã©vite saturation)
export OLLAMA_MAX_LOADED_MODELS=1   # 1 modÃ¨le en mÃ©moire max
export OLLAMA_NUM_THREADS=4         # 4 threads (i5-6300U = 2 cores, 4 threads)
```

### LangChain Batch Processing

```python
# Ã‰viter appels sÃ©quentiels â†’ utiliser batch
from langchain_core.runnables import RunnableBatch

# Mauvais (lent sur CPU)
for task in tasks:
    result = await agent.process(task)

# Bon (batch parallÃ¨le limitÃ©)
batch = RunnableBatch([agent.chain for _ in tasks])
results = await batch.abatch([{"input": task} for task in tasks], max_concurrency=2)
```

### Cache RÃ©ponses FrÃ©quentes

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_response(query: str) -> str:
    """Cache les 100 derniÃ¨res rÃ©ponses."""
    # Si query dÃ©jÃ  vue, retourne rÃ©ponse cached (Ã©vite appel Ollama)
    pass
```
</optimisations_cpu>

---

**Version 1.0 | 2025-12-11 | TAKUMI Best Practices Shizen-Koshin**
