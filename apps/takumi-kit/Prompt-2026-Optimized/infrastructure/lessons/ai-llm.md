# Lessons Learned - IA & LLM

> LeÃ§ons apprises liÃ©es Ã  l'IA, LLM, Ollama, RAG, embeddings.

---

## ðŸ“Š Statistiques

**LeÃ§ons documentÃ©es** : 1
**DerniÃ¨re mise Ã  jour** : 2026-01-26

---

## LeÃ§ons

### [IA] [OLLAMA] ModÃ¨le pas chargÃ©, timeout
**Date** : 2026-01-12 | **Projet** : Shizen-Koshin-MVP | **SÃ©vÃ©ritÃ©** : ðŸŸ¡

**Contexte** :
Premier appel au modÃ¨le trÃ¨s lent (timeout).

**Erreur** :
Ollama charge le modÃ¨le en mÃ©moire au premier appel.

**Solution** :
```python
# Warmup au dÃ©marrage de l'app
async def warmup_model():
    try:
        await llm.ainvoke("test")  # Premier appel charge le modÃ¨le
    except:
        pass  # Ignorer erreur warmup

# Au startup
asyncio.create_task(warmup_model())
```

**PrÃ©vention** :
1. Warmup au dÃ©marrage de l'application
2. Timeout gÃ©nÃ©reux pour premier appel (60s+)
3. VÃ©rifier que Ollama tourne : `curl http://localhost:11434/api/tags`

**Fichiers/Commandes ClÃ©s** :
- `curl http://localhost:11434/api/tags` - VÃ©rifier Ollama
- `ollama list` - Lister modÃ¨les installÃ©s
- `ollama pull llama2` - TÃ©lÃ©charger modÃ¨le

---

## ðŸ’¡ Patterns Communs

### Pattern 1 : Warmup Ollama
```python
import asyncio
from langchain.llms import Ollama

async def warmup_ollama(model_name: str = "llama2"):
    """Warmup Ollama model au dÃ©marrage"""
    try:
        llm = Ollama(model=model_name)
        _ = await llm.ainvoke("test", timeout=60)
        print(f"âœ… Ollama {model_name} warmed up")
    except Exception as e:
        print(f"âš ï¸ Warmup failed: {e}")

# Au dÃ©marrage
asyncio.create_task(warmup_ollama())
```

### Pattern 2 : RAG avec Retry
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def query_rag(question: str):
    """Query RAG avec retry automatique"""
    docs = retriever.get_relevant_documents(question)
    context = "\n".join([doc.page_content for doc in docs])

    response = await llm.ainvoke(
        f"Context: {context}\n\nQuestion: {question}"
    )
    return response
```

### Pattern 3 : Chunking Documents
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def chunk_document(text: str, chunk_size: int = 1000, overlap: int = 200):
    """Chunk document pour RAG"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = splitter.split_text(text)
    return chunks
```

---

## ðŸ¤– Checklist LLM Integration

- [ ] Warmup model au dÃ©marrage
- [ ] Timeout gÃ©nÃ©reux (60s+)
- [ ] Retry logic pour robustesse
- [ ] Streaming pour UX (si applicable)
- [ ] Rate limiting (Ã©viter surcharge)
- [ ] Logging prompts + responses
- [ ] Error handling graceful
- [ ] Fallback si LLM down
- [ ] Monitoring performance (latence)
- [ ] Cost tracking (si API externe)

---

## ðŸ“Š Performance Ollama

| ModÃ¨le | Taille | RAM Requis | Vitesse | Use Case |
|--------|--------|------------|---------|----------|
| llama2:7b | 3.8GB | 8GB | Rapide | Chat, QA |
| llama2:13b | 7.3GB | 16GB | Moyen | QualitÃ© > Vitesse |
| mistral:7b | 4.1GB | 8GB | Rapide | Code, reasoning |
| codellama:7b | 3.8GB | 8GB | Rapide | Code generation |

---

## ðŸ”— Voir Aussi

- Infrastructure: [LOCAL-AI-INFRA.md](../LOCAL-AI-INFRA.md)
- Projects: [Shizen-Koshin-MVP](../../../Shizen-Koshin-MVP/)

---

**Maintenu par** : TAKUMI (Claude Code)
**Template version** : 1.0
