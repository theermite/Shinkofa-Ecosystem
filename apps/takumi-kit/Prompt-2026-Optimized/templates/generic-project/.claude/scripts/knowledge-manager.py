#!/usr/bin/env python3
"""
Knowledge Library Manager

G√®re la base de connaissances personnalis√©e pour enrichir le contexte Claude.

Workflow 5-phase:
1. DISCOVER  - Scanner documents disponibles
2. CONFIGURE - Configurer cat√©gories et m√©tadonn√©es
3. INGEST    - Extraire texte, chunker, indexer
4. ENRICH    - Tags auto, relations, embeddings
5. GENERATE  - Consultation automatique (int√©gr√© Claude)

Usage:
    python knowledge-manager.py init
    python knowledge-manager.py ingest <file> --category <cat>
    python knowledge-manager.py search <query>
    python knowledge-manager.py stats
"""

import sys
import os
import json
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from collections import Counter


# ==============================================
# Configuration
# ==============================================

DEFAULT_CHUNK_SIZE = 800
DEFAULT_CHUNK_OVERLAP = 100

SUPPORTED_FORMATS = {
    '.md': 'markdown',
    '.txt': 'text',
    '.pdf': 'pdf',  # N√©cessite PyPDF2 ou pdfplumber
    '.docx': 'docx'  # N√©cessite python-docx
}

DEFAULT_CATEGORIES = {
    'coaching': 'Frameworks et m√©thodologies coaching',
    'business': 'Business plan, strat√©gie, master plan',
    'technical': 'Architecture et d√©cisions techniques'
}


# ==============================================
# Helpers
# ==============================================

def get_project_root() -> Path:
    """Trouve la racine du projet (o√π se trouve .claude/)."""
    current = Path.cwd()
    while current != current.parent:
        if (current / ".claude").exists():
            return current
        current = current.parent
    return Path.cwd()


def get_knowledge_dir() -> Path:
    """Retourne le chemin vers .claude/knowledge/."""
    return get_project_root() / ".claude" / "knowledge"


def get_config_path() -> Path:
    """Retourne chemin config.json."""
    return get_knowledge_dir() / "config.json"


def get_index_path() -> Path:
    """Retourne chemin index.json."""
    return get_knowledge_dir() / "index.json"


def load_config() -> Dict:
    """Charge configuration."""
    config_path = get_config_path()
    if not config_path.exists():
        return create_default_config()

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lecture config: {e}")
        return create_default_config()


def save_config(config: Dict):
    """Sauvegarde configuration."""
    config_path = get_config_path()
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde config: {e}")


def create_default_config() -> Dict:
    """Cr√©e configuration par d√©faut."""
    return {
        "version": "1.0",
        "chunk_size": DEFAULT_CHUNK_SIZE,
        "chunk_overlap": DEFAULT_CHUNK_OVERLAP,
        "categories": {
            cat: {"description": desc, "enabled": True, "auto_tags": True}
            for cat, desc in DEFAULT_CATEGORIES.items()
        },
        "auto_enrich": True,
        "embeddings": {
            "enabled": False,
            "provider": "openai",
            "model": "text-embedding-3-small"
        }
    }


def load_index() -> Dict:
    """Charge index."""
    index_path = get_index_path()
    if not index_path.exists():
        return {"documents": [], "chunks": [], "stats": {}}

    try:
        with open(index_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lecture index: {e}")
        return {"documents": [], "chunks": [], "stats": {}}


def save_index(index: Dict):
    """Sauvegarde index."""
    index_path = get_index_path()
    try:
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde index: {e}")


def file_hash(file_path: Path) -> str:
    """Calcule hash MD5 d'un fichier."""
    md5 = hashlib.md5()
    try:
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                md5.update(chunk)
        return md5.hexdigest()
    except Exception:
        return ""


# ==============================================
# Phase 1: DISCOVER
# ==============================================

def discover_documents(path: str, recursive: bool = False) -> List[Path]:
    """
    Scanner documents dans un chemin.

    Args:
        path: Chemin dossier ou fichier
        recursive: Recherche r√©cursive

    Returns:
        Liste de fichiers support√©s trouv√©s
    """
    path_obj = Path(path).expanduser().resolve()

    if not path_obj.exists():
        print(f"‚ùå Chemin introuvable: {path}")
        return []

    files = []

    if path_obj.is_file():
        if path_obj.suffix in SUPPORTED_FORMATS:
            files.append(path_obj)
    elif path_obj.is_dir():
        pattern = "**/*" if recursive else "*"
        for ext in SUPPORTED_FORMATS:
            files.extend(path_obj.glob(f"{pattern}{ext}"))

    return files


def cmd_discover(path: str, recursive: bool = False):
    """Commande discover."""
    print(f"üîç D√©couverte documents dans: {path}\n")

    files = discover_documents(path, recursive)

    if not files:
        print("‚ÑπÔ∏è Aucun document trouv√©")
        return

    # Grouper par type
    by_type = {}
    for f in files:
        ftype = SUPPORTED_FORMATS.get(f.suffix, 'unknown')
        by_type.setdefault(ftype, []).append(f)

    print(f"üìÑ {len(files)} documents trouv√©s\n")

    for ftype, file_list in sorted(by_type.items()):
        print(f"  {ftype}: {len(file_list)} fichiers")
        for f in file_list[:5]:  # Max 5 exemples
            print(f"    - {f.name}")
        if len(file_list) > 5:
            print(f"    ... et {len(file_list) - 5} autres")
        print()

    print(f"üí° Pour ing√©rer:")
    print(f"   python knowledge-manager.py ingest {path} --category [category]")


# ==============================================
# Phase 2: CONFIGURE
# ==============================================

def cmd_init():
    """Initialise structure Knowledge Library."""
    knowledge_dir = get_knowledge_dir()

    print("üöÄ Initialisation Knowledge Library\n")

    # Cr√©er r√©pertoire principal
    knowledge_dir.mkdir(parents=True, exist_ok=True)
    print(f"‚úÖ Cr√©√©: {knowledge_dir}")

    # Cr√©er cat√©gories par d√©faut
    for category in DEFAULT_CATEGORIES:
        cat_dir = knowledge_dir / category
        cat_dir.mkdir(exist_ok=True)
        print(f"‚úÖ Cr√©√© cat√©gorie: {category}/")

    # Cr√©er config
    config = create_default_config()
    save_config(config)
    print(f"‚úÖ Cr√©√©: config.json")

    # Cr√©er index vide
    index = {"documents": [], "chunks": [], "stats": {}}
    save_index(index)
    print(f"‚úÖ Cr√©√©: index.json")

    print("\nüéâ Knowledge Library initialis√©e !")
    print(f"\nüìÅ Structure cr√©√©e dans: {knowledge_dir}")
    print("\nüí° Prochaines √©tapes:")
    print("   1. Ajouter documents dans les cat√©gories")
    print("   2. Ing√©rer: python knowledge-manager.py ingest <file> --category <cat>")
    print("   3. Rechercher: python knowledge-manager.py search <query>")


def cmd_add_category(name: str, description: str):
    """Ajoute une cat√©gorie."""
    knowledge_dir = get_knowledge_dir()
    config = load_config()

    # Cr√©er dossier
    cat_dir = knowledge_dir / name
    cat_dir.mkdir(parents=True, exist_ok=True)

    # Ajouter √† config
    config['categories'][name] = {
        "description": description,
        "enabled": True,
        "auto_tags": True
    }
    save_config(config)

    print(f"‚úÖ Cat√©gorie '{name}' cr√©√©e")
    print(f"üìÅ {cat_dir}")


# ==============================================
# Phase 3: INGEST
# ==============================================

def extract_text(file_path: Path) -> str:
    """Extrait texte d'un fichier."""
    suffix = file_path.suffix

    if suffix == '.md' or suffix == '.txt':
        try:
            return file_path.read_text(encoding='utf-8')
        except Exception as e:
            print(f"‚ùå Erreur lecture {file_path}: {e}")
            return ""

    elif suffix == '.pdf':
        try:
            import PyPDF2
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except ImportError:
            print("‚ö†Ô∏è PyPDF2 non install√©: pip install PyPDF2")
            return ""
        except Exception as e:
            print(f"‚ùå Erreur extraction PDF: {e}")
            return ""

    elif suffix == '.docx':
        try:
            import docx
            doc = docx.Document(file_path)
            return "\n".join([para.text for para in doc.paragraphs])
        except ImportError:
            print("‚ö†Ô∏è python-docx non install√©: pip install python-docx")
            return ""
        except Exception as e:
            print(f"‚ùå Erreur extraction DOCX: {e}")
            return ""

    return ""


def extract_frontmatter(text: str) -> Tuple[Dict, str]:
    """Extrait frontmatter YAML d'un document Markdown."""
    frontmatter = {}
    content = text

    # Pattern frontmatter YAML
    pattern = r'^---\s*\n(.*?)\n---\s*\n'
    match = re.match(pattern, text, re.DOTALL)

    if match:
        yaml_text = match.group(1)
        content = text[match.end():]

        # Parse YAML simple (pas de d√©pendance PyYAML)
        for line in yaml_text.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()

                # Parse listes [a, b, c]
                if value.startswith('[') and value.endswith(']'):
                    value = [v.strip() for v in value[1:-1].split(',')]

                frontmatter[key] = value

    return frontmatter, content


def chunk_text(text: str, chunk_size: int = DEFAULT_CHUNK_SIZE,
               overlap: int = DEFAULT_CHUNK_OVERLAP) -> List[str]:
    """
    D√©coupe texte en chunks avec overlap.

    Args:
        text: Texte √† d√©couper
        chunk_size: Taille chunk en caract√®res
        overlap: Overlap entre chunks

    Returns:
        Liste de chunks
    """
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # Essayer de couper √† un espace ou newline
        if end < len(text):
            # Chercher dernier espace/newline dans les 100 derniers chars
            search_start = max(end - 100, start)
            last_break = max(
                text.rfind(' ', search_start, end),
                text.rfind('\n', search_start, end)
            )
            if last_break > start:
                end = last_break

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        start = end - overlap

    return chunks


def ingest_document(file_path: Path, category: str,
                    tags: List[str] = None, author: str = None) -> bool:
    """
    Ing√®re un document dans la Knowledge Library.

    Args:
        file_path: Chemin fichier
        category: Cat√©gorie (ex: "coaching/frameworks")
        tags: Tags additionnels
        author: Auteur

    Returns:
        True si succ√®s
    """
    if not file_path.exists():
        print(f"‚ùå Fichier introuvable: {file_path}")
        return False

    if file_path.suffix not in SUPPORTED_FORMATS:
        print(f"‚ùå Format non support√©: {file_path.suffix}")
        return False

    config = load_config()
    index = load_index()

    # Extraire texte
    print(f"üìÑ Extraction: {file_path.name}")
    text = extract_text(file_path)

    if not text:
        print(f"‚ùå Aucun texte extrait de {file_path}")
        return False

    # Extraire frontmatter (si Markdown)
    frontmatter, content = extract_frontmatter(text)

    # M√©tadonn√©es
    doc_hash = file_hash(file_path)
    doc_id = f"{category}/{file_path.stem}"

    metadata = {
        "id": doc_id,
        "file_path": str(file_path),
        "file_name": file_path.name,
        "category": category,
        "format": SUPPORTED_FORMATS[file_path.suffix],
        "hash": doc_hash,
        "size": len(text),
        "added": datetime.now().isoformat(),
        "last_updated": datetime.now().isoformat(),
        "tags": tags or frontmatter.get('tags', []),
        "author": author or frontmatter.get('author', ''),
        "title": frontmatter.get('title', file_path.stem)
    }

    # Chunker texte
    chunk_size = config['chunk_size']
    overlap = config['chunk_overlap']

    print(f"üî™ Chunking (size={chunk_size}, overlap={overlap})")
    chunks = chunk_text(content, chunk_size, overlap)
    print(f"‚úÖ {len(chunks)} chunks cr√©√©s")

    # Ajouter au index
    # V√©rifier si doc existe d√©j√†
    existing = next((d for d in index['documents'] if d['id'] == doc_id), None)

    if existing:
        print(f"‚ö†Ô∏è Document existe d√©j√†, mise √† jour")
        # Supprimer anciens chunks
        index['chunks'] = [c for c in index['chunks'] if c['doc_id'] != doc_id]
        # Mettre √† jour metadata
        existing.update(metadata)
    else:
        # Nouveau document
        index['documents'].append(metadata)

    # Ajouter chunks
    for i, chunk in enumerate(chunks):
        chunk_data = {
            "id": f"{doc_id}_chunk_{i}",
            "doc_id": doc_id,
            "index": i,
            "text": chunk,
            "size": len(chunk)
        }
        index['chunks'].append(chunk_data)

    # Mettre √† jour stats
    index['stats'] = {
        "total_documents": len(index['documents']),
        "total_chunks": len(index['chunks']),
        "total_size": sum(d['size'] for d in index['documents']),
        "last_updated": datetime.now().isoformat()
    }

    save_index(index)

    print(f"‚úÖ Document ing√©r√©: {doc_id}")
    return True


def cmd_ingest(path: str, category: str, tags: str = None,
               author: str = None, recursive: bool = False):
    """Commande ingest."""
    print(f"üì• Ingestion documents\n")

    # D√©couvrir fichiers
    files = discover_documents(path, recursive)

    if not files:
        print("‚ùå Aucun fichier √† ing√©rer")
        return

    print(f"üìÑ {len(files)} fichiers trouv√©s\n")

    # Parser tags
    tag_list = [t.strip() for t in tags.split(',')] if tags else []

    # Ing√©rer chaque fichier
    success = 0
    for file in files:
        if ingest_document(file, category, tag_list, author):
            success += 1
        print()

    print(f"üéâ {success}/{len(files)} documents ing√©r√©s avec succ√®s")


# ==============================================
# Phase 4: ENRICH
# ==============================================

def auto_generate_tags(text: str) -> List[str]:
    """G√©n√®re tags automatiquement depuis le texte (simple NLP)."""
    # Mots-cl√©s fr√©quents (excluant stop words)
    stop_words = {'le', 'la', 'les', 'un', 'une', 'des', 'et', 'ou', 'mais',
                  'de', 'du', 'pour', 'dans', 'sur', 'avec', 'est', 'sont'}

    words = re.findall(r'\b\w+\b', text.lower())
    words = [w for w in words if len(w) > 3 and w not in stop_words]

    # Top 5 mots les plus fr√©quents
    counter = Counter(words)
    return [word for word, count in counter.most_common(5)]


def cmd_enrich():
    """Enrichit tous les documents avec tags auto."""
    print("üé® Enrichissement documents\n")

    index = load_index()

    for doc in index['documents']:
        # R√©cup√©rer texte depuis chunks
        doc_chunks = [c['text'] for c in index['chunks'] if c['doc_id'] == doc['id']]
        full_text = " ".join(doc_chunks)

        # G√©n√©rer tags auto si pas d√©j√† pr√©sents
        if not doc.get('tags'):
            auto_tags = auto_generate_tags(full_text)
            doc['tags'] = auto_tags
            print(f"‚úÖ {doc['id']}: tags = {auto_tags}")

    # Sauvegarder
    save_index(index)
    print("\nüéâ Enrichissement termin√©")


# ==============================================
# Recherche & Stats
# ==============================================

def search_knowledge(query: str, category: str = None,
                     limit: int = 5, context: int = 200) -> List[Dict]:
    """
    Recherche dans la Knowledge Library.

    Args:
        query: Requ√™te recherche
        category: Filtrer par cat√©gorie (optionnel)
        limit: Nombre max r√©sultats
        context: Taille contexte autour du match (chars)

    Returns:
        Liste de r√©sultats avec scores
    """
    index = load_index()
    query_lower = query.lower()

    results = []

    for chunk in index['chunks']:
        # Filtrer par cat√©gorie
        if category:
            doc = next((d for d in index['documents'] if d['id'] == chunk['doc_id']), None)
            if not doc or not doc['category'].startswith(category):
                continue

        # Recherche simple (contains)
        if query_lower in chunk['text'].lower():
            # Score basique = nombre d'occurrences
            score = chunk['text'].lower().count(query_lower)

            # Contexte autour du match
            idx = chunk['text'].lower().find(query_lower)
            start = max(0, idx - context)
            end = min(len(chunk['text']), idx + len(query) + context)
            snippet = chunk['text'][start:end]

            if start > 0:
                snippet = "..." + snippet
            if end < len(chunk['text']):
                snippet = snippet + "..."

            doc = next((d for d in index['documents'] if d['id'] == chunk['doc_id']), None)

            results.append({
                "doc_id": chunk['doc_id'],
                "doc_title": doc['title'] if doc else chunk['doc_id'],
                "category": doc['category'] if doc else "",
                "chunk_id": chunk['id'],
                "score": score,
                "snippet": snippet
            })

    # Trier par score
    results.sort(key=lambda x: x['score'], reverse=True)

    return results[:limit]


def cmd_search(query: str, category: str = None, limit: int = 5):
    """Commande search."""
    print(f"üîç Recherche: '{query}'\n")

    if category:
        print(f"üìÅ Cat√©gorie: {category}\n")

    results = search_knowledge(query, category, limit)

    if not results:
        print("‚ÑπÔ∏è Aucun r√©sultat trouv√©")
        return

    print(f"üìÑ {len(results)} r√©sultats trouv√©s\n")

    for i, result in enumerate(results, 1):
        print(f"{i}. {result['doc_title']}")
        print(f"   üìÅ {result['category']}")
        print(f"   üìä Score: {result['score']}")
        print(f"   üìù {result['snippet']}")
        print()


def cmd_stats():
    """Affiche statistiques Knowledge Library."""
    index = load_index()
    config = load_config()

    stats = index.get('stats', {})

    print("üìä Knowledge Library Statistics\n")
    print(f"Documents: {stats.get('total_documents', 0)}")
    print(f"Chunks: {stats.get('total_chunks', 0)}")
    print(f"Total Size: {stats.get('total_size', 0):,} chars")
    print(f"Last Updated: {stats.get('last_updated', 'Never')}\n")

    # Stats par cat√©gorie
    by_category = {}
    for doc in index['documents']:
        cat = doc['category']
        by_category[cat] = by_category.get(cat, 0) + 1

    if by_category:
        print("üìÅ By Category:")
        for cat, count in sorted(by_category.items(), key=lambda x: x[1], reverse=True):
            print(f"   {cat}: {count} docs")
        print()

    # Top tags
    all_tags = []
    for doc in index['documents']:
        all_tags.extend(doc.get('tags', []))

    if all_tags:
        print("üè∑Ô∏è Top Tags:")
        counter = Counter(all_tags)
        for tag, count in counter.most_common(10):
            print(f"   {tag}: {count}")
        print()

    # Config
    print("‚öôÔ∏è Configuration:")
    print(f"   Chunk Size: {config['chunk_size']}")
    print(f"   Overlap: {config['chunk_overlap']}")
    print(f"   Auto Enrich: {config['auto_enrich']}")
    print(f"   Embeddings: {config['embeddings']['enabled']}")


# ==============================================
# Main
# ==============================================

def show_help():
    print("""
Knowledge Library Manager

Usage:
  python knowledge-manager.py <command> [options]

Commands:
  init                                  Initialize Knowledge Library
  discover <path> [--recursive]         Discover documents
  ingest <path> --category <cat>        Ingest documents
         [--tags <tags>]                Tags (comma-separated)
         [--author <author>]            Author
         [--recursive]                  Recursive
  search <query> [--category <cat>]     Search knowledge
         [--limit N]                    Max results (default 5)
  stats                                 Show statistics
  enrich                                Auto-generate tags

Examples:
  python knowledge-manager.py init
  python knowledge-manager.py discover ~/Documents/Coaching
  python knowledge-manager.py ingest doc.md --category coaching/frameworks
  python knowledge-manager.py ingest ~/Docs/*.md --category business --tags "voschinkoff,plan"
  python knowledge-manager.py search "design humain" --category coaching
  python knowledge-manager.py stats
""")


def main():
    if len(sys.argv) < 2:
        show_help()
        sys.exit(1)

    command = sys.argv[1]

    if command == "help" or command == "-h" or command == "--help":
        show_help()

    elif command == "init":
        cmd_init()

    elif command == "discover":
        if len(sys.argv) < 3:
            print("‚ùå Usage: python knowledge-manager.py discover <path> [--recursive]")
            sys.exit(1)
        path = sys.argv[2]
        recursive = "--recursive" in sys.argv
        cmd_discover(path, recursive)

    elif command == "ingest":
        if len(sys.argv) < 3:
            print("‚ùå Usage: python knowledge-manager.py ingest <path> --category <category>")
            sys.exit(1)

        path = sys.argv[2]

        # Parse arguments
        args = sys.argv[3:]
        category = None
        tags = None
        author = None
        recursive = False

        i = 0
        while i < len(args):
            if args[i] == "--category" and i + 1 < len(args):
                category = args[i + 1]
                i += 2
            elif args[i] == "--tags" and i + 1 < len(args):
                tags = args[i + 1]
                i += 2
            elif args[i] == "--author" and i + 1 < len(args):
                author = args[i + 1]
                i += 2
            elif args[i] == "--recursive":
                recursive = True
                i += 1
            else:
                i += 1

        if not category:
            print("‚ùå --category requis")
            sys.exit(1)

        cmd_ingest(path, category, tags, author, recursive)

    elif command == "search":
        if len(sys.argv) < 3:
            print("‚ùå Usage: python knowledge-manager.py search <query> [--category <cat>]")
            sys.exit(1)

        query = sys.argv[2]
        category = None
        limit = 5

        # Parse arguments
        args = sys.argv[3:]
        i = 0
        while i < len(args):
            if args[i] == "--category" and i + 1 < len(args):
                category = args[i + 1]
                i += 2
            elif args[i] == "--limit" and i + 1 < len(args):
                limit = int(args[i + 1])
                i += 2
            else:
                i += 1

        cmd_search(query, category, limit)

    elif command == "stats":
        cmd_stats()

    elif command == "enrich":
        cmd_enrich()

    else:
        print(f"‚ùå Commande inconnue: {command}")
        print("Utilise 'help' pour voir les commandes disponibles")
        sys.exit(1)


if __name__ == "__main__":
    main()
